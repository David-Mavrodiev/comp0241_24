{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extract the Astronomical Object from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def align_images_multiple_references(reference_image_paths, distorted_image_path):\n",
    "    # Load the distorted image\n",
    "    dist_img = cv2.imread(distorted_image_path)\n",
    "    dist_gray = cv2.cvtColor(dist_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Aggregate keypoints and descriptors from all reference images\n",
    "    ref_keypoints = []\n",
    "    ref_descriptors = []\n",
    "    for ref_path in reference_image_paths:\n",
    "        ref_img = cv2.imread(ref_path)\n",
    "        ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)\n",
    "        keypoints, descriptors = sift.detectAndCompute(ref_gray, None)\n",
    "        ref_keypoints.extend(keypoints)\n",
    "        if descriptors is not None:\n",
    "            if len(ref_descriptors) == 0:\n",
    "                ref_descriptors = descriptors\n",
    "            else:\n",
    "                ref_descriptors = np.vstack((ref_descriptors, descriptors))\n",
    "\n",
    "    # Detect keypoints and descriptors in the distorted image\n",
    "    keypoints_dist, descriptors_dist = sift.detectAndCompute(dist_gray, None)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(ref_descriptors, descriptors_dist)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    ref_pts = np.float32([ref_keypoints[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dist_pts = np.float32([keypoints_dist[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate homography\n",
    "    matrix, mask = cv2.findHomography(dist_pts, ref_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp the distorted image\n",
    "    h, w = dist_img.shape[:2]\n",
    "    aligned_image = cv2.warpPerspective(dist_img, matrix, (w, h))\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Distorted Image\")\n",
    "    plt.imshow(cv2.cvtColor(dist_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Aligned Image\")\n",
    "    plt.imshow(cv2.cvtColor(aligned_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def transform_image_with_homography(image_path, homography_matrix):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w = image.shape[:2]  # Get the size from the image itself\n",
    "\n",
    "    # Apply the homography transformation\n",
    "    transformed_image = cv2.warpPerspective(image, homography_matrix, (w, h))\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "def reverse_transform_image_with_homography(transformed_image, homography_matrix):\n",
    "    \"\"\"\n",
    "    Reverse the homography transformation on an image.\n",
    "\n",
    "    Parameters:\n",
    "        transformed_image: The image that was transformed.\n",
    "        homography_matrix: The homography matrix used for the forward transformation.\n",
    "\n",
    "    Returns:\n",
    "        The reversed (original) image.\n",
    "    \"\"\"\n",
    "    # Calculate the inverse of the homography matrix\n",
    "    inverse_homography_matrix = np.linalg.inv(homography_matrix)\n",
    "    \n",
    "    # Get the size of the transformed image\n",
    "    h, w = transformed_image.shape[:2]\n",
    "    \n",
    "    # Apply the inverse homography transformation\n",
    "    original_image = cv2.warpPerspective(transformed_image, inverse_homography_matrix, (w, h))\n",
    "    \n",
    "    return original_image\n",
    "\n",
    "def save_homography_matrix(matrix, file_path):\n",
    "    # Convert the NumPy array to a list for JSON serialization\n",
    "    matrix_list = matrix.tolist()\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(matrix_list, f)\n",
    "\n",
    "def process_dataset_images(dataset_folder):\n",
    "    images_folder = os.path.join(dataset_folder, \"images\")\n",
    "    \n",
    "    for image_name in os.listdir(images_folder):\n",
    "        print(image_name)\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "\n",
    "        reference_image_paths = [\n",
    "            './Dataset 2/images/000003.png', './Dataset 2/images/000010.png', './Dataset 2/images/000016.png', './Dataset 2/images/000021.png', './Dataset 2/images/000022.png', './Dataset 2/images/000028.png', './Dataset 2/images/000030.png', './Dataset 2/images/000033.png', './Dataset 2/images/000046.png', './Dataset 2/images/000051.png', './Dataset 2/images/000053.png', './Dataset 2/images/000060.png', './Dataset 2/images/000061.png', './Dataset 2/images/000064.png',\n",
    "            './Dataset 2/images/000072.png', './Dataset 2/images/000081.png', './Dataset 2/images/000096.png', \n",
    "        ]  # Add more references\n",
    "        #distorted_image_path = './Dataset 2/images/000083.png'\n",
    "        homography_matrix = align_images_multiple_references(reference_image_paths, image_path)\n",
    "\n",
    "        # Transform the image\n",
    "        transformed_image = transform_image_with_homography(image_path, homography_matrix)\n",
    "\n",
    "        if not cv2.imwrite(f\"./calibrated-images/{image_name}\", transformed_image):\n",
    "            print(f\"Failed to save image: {image_name}\")\n",
    "\n",
    "        save_homography_matrix(homography_matrix, f\"./calibrated-images/{image_name.replace('png', 'json')}\")\n",
    "\n",
    "        # Plot the original and transformed images\n",
    "        #original_image = cv2.imread(image_path)\n",
    "        #plt.figure(figsize=(12, 6))\n",
    "        #plt.subplot(1, 2, 1)\n",
    "        #plt.title(\"Original Image\")\n",
    "        #plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        #plt.axis(\"off\")\n",
    "\n",
    "        #plt.subplot(1, 2, 2)\n",
    "        #plt.title(\"Transformed Image\")\n",
    "        #plt.imshow(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "        #plt.axis(\"off\")\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        #plt.show()\n",
    "\n",
    "# Example usage\n",
    "dataset_folder = './Dataset 2'  # Replace with the actual dataset folder\n",
    "process_dataset_images(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_homography_matrix(file_path):\n",
    "    # Load the JSON file and convert the list back to a NumPy array\n",
    "    with open(file_path, 'r') as f:\n",
    "        matrix_list = json.load(f)\n",
    "    return np.array(matrix_list)\n",
    "\n",
    "def reverse_transform_image_with_homography(transformed_image, homography_matrix):\n",
    "    \"\"\"\n",
    "    Reverse the homography transformation on an image.\n",
    "\n",
    "    Parameters:\n",
    "        transformed_image: The image that was transformed.\n",
    "        homography_matrix: The homography matrix used for the forward transformation.\n",
    "\n",
    "    Returns:\n",
    "        The reversed (original) image.\n",
    "    \"\"\"\n",
    "    # Calculate the inverse of the homography matrix\n",
    "    inverse_homography_matrix = np.linalg.inv(homography_matrix)\n",
    "    \n",
    "    # Get the size of the transformed image\n",
    "    h, w = transformed_image.shape[:2]\n",
    "    \n",
    "    # Apply the inverse homography transformation\n",
    "    original_image = cv2.warpPerspective(transformed_image, inverse_homography_matrix, (w, h))\n",
    "    \n",
    "    return original_image\n",
    "\n",
    "def color_thresholding(image, lower_bounds, upper_bounds):\n",
    "    \"\"\"\n",
    "    Perform color thresholding to segment the globe.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        lower_bounds (list of numpy.ndarray): List of lower HSV color bounds.\n",
    "        upper_bounds (list of numpy.ndarray): List of upper HSV color bounds.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask highlighting the selected colors.\n",
    "    \"\"\"\n",
    "    # Convert image to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = np.zeros(hsv_image.shape[:2], dtype=np.uint8)\n",
    "    # Apply each color threshold and combine the masks\n",
    "    for lower_bound, upper_bound in zip(lower_bounds, upper_bounds):\n",
    "        mask |= cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "    return mask\n",
    "\n",
    "def refined_circle_detection_with_mask(image, color_mask):\n",
    "    \"\"\"\n",
    "    Detect a single circular shape in the image using a color mask to focus the detection.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        color_mask (numpy.ndarray): Binary mask obtained from color thresholding.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask with the detected circle.\n",
    "    \"\"\"\n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=color_mask)\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (9, 9), 2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(blurred_image, cmap='gray')\n",
    "    plt.title(f\"Thresholding Result {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    # Detect circles using HoughCircles with adjusted parameters\n",
    "    circles = cv2.HoughCircles(\n",
    "        blurred_image,\n",
    "        cv2.HOUGH_GRADIENT,\n",
    "        dp=0.2,\n",
    "        minDist=200,\n",
    "        param1=150,\n",
    "        param2=40,\n",
    "        minRadius=30,\n",
    "        maxRadius=400,\n",
    "    )\n",
    "\n",
    "    # Create a binary mask with the same dimensions as the input image\n",
    "    mask = np.zeros_like(gray_image)\n",
    "    if circles is not None:\n",
    "        # Get the first (most prominent) circle\n",
    "        circle = np.round(circles[0, 0]).astype(\"int\")\n",
    "        x, y, r = circle\n",
    "        # Draw the circle on the mask\n",
    "        cv2.circle(mask, (x, y), r, 255, -1)\n",
    "    return mask\n",
    "\n",
    "def compute_roc_curve(predicted_mask, ground_truth_mask):\n",
    "    \"\"\"\n",
    "    Compute ROC curve and AUC for a predicted mask and ground truth mask.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_mask (numpy.ndarray): Predicted binary mask.\n",
    "        ground_truth_mask (numpy.ndarray): Ground truth binary mask.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fpr, tpr, roc_auc) False Positive Rate, True Positive Rate, and Area Under Curve.\n",
    "    \"\"\"\n",
    "    # Binarize the masks\n",
    "    predicted_binary = (predicted_mask > 0).astype(np.uint8).flatten()\n",
    "    ground_truth_binary = (ground_truth_mask > 0).astype(np.uint8).flatten()\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(ground_truth_binary, predicted_binary)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "# Dataset paths\n",
    "images_folder = './Dataset 2/images/'\n",
    "corrected_images_folder = './calibrated-images/'\n",
    "masks_folder = './Dataset 2/masks/'\n",
    "\n",
    "# Bounds for color thresholding\n",
    "lower_bounds = [\n",
    "    np.array([100, 50, 50]),  # Lower bound of blue in HSV\n",
    "    np.array([0, 0, 200]),    # Lower bound of white in HSV\n",
    "]\n",
    "upper_bounds = [\n",
    "    np.array([140, 255, 255]),  # Upper bound of blue in HSV\n",
    "    np.array([180, 50, 255]),   # Upper bound of white in HSV\n",
    "]\n",
    "\n",
    "# Lists to store ROC values\n",
    "all_fpr = []\n",
    "all_tpr = []\n",
    "all_auc = []\n",
    "\n",
    "# Loop through all images and masks\n",
    "for i in range(100):\n",
    "    image_path = os.path.join(images_folder, f\"{i:06d}.png\")\n",
    "    corrected_image_path = os.path.join(corrected_images_folder, f\"{i:06d}.png\")\n",
    "    mask_path = os.path.join(masks_folder, f\"{i:06d}.png\")\n",
    "\n",
    "    # Read image and ground truth mask\n",
    "    image = cv2.imread(image_path)\n",
    "    corrected_image = cv2.imread(corrected_image_path)\n",
    "    ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    loaded_matrix = load_homography_matrix(corrected_image_path.replace(\"png\", \"json\"))\n",
    "\n",
    "    # Generate color mask\n",
    "    color_mask = color_thresholding(corrected_image, lower_bounds, upper_bounds)\n",
    "\n",
    "    # Plot the thresholding result\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(color_mask, cmap='gray')\n",
    "    plt.title(f\"Thresholding Result {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Refine circle detection with color mask\n",
    "    predicted_mask = reverse_transform_image_with_homography(refined_circle_detection_with_mask(image, color_mask), loaded_matrix)\n",
    "\n",
    "    if not cv2.imwrite(f\"./calibrated-images/{i:06d}_predicted_mask.png\", predicted_mask):\n",
    "            print(f\"Failed to save {i:06d}_predicted_mask.png\")\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, roc_auc = compute_roc_curve(predicted_mask, ground_truth_mask)\n",
    "\n",
    "    # Store the results\n",
    "    all_fpr.append(fpr)\n",
    "    all_tpr.append(tpr)\n",
    "    all_auc.append(roc_auc)\n",
    "\n",
    "    # Plot image, ground truth mask, and predicted mask\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Image {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ground_truth_mask, cmap='gray')\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predicted_mask, cmap='gray')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute average ROC curve (mean TPR for each FPR)\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for fpr, tpr in zip(all_fpr, all_tpr):\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "mean_tpr /= len(all_tpr)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "# Plot average ROC curve\n",
    "plt.figure()\n",
    "plt.plot(mean_fpr, mean_tpr, label=f\"Mean ROC (AUC = {mean_auc:.2f})\")\n",
    "plt.title(\"Average ROC Curve for Test Set\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print mean AUC\n",
    "print(f\"Mean AUC: {mean_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dataset paths\n",
    "images_folder = './Dataset 2/images/'\n",
    "masks_folder = './calibrated-images/'\n",
    "\n",
    "def find_mask_center(mask):\n",
    "    # Compute the coordinates of the white pixels\n",
    "    white_pixels = np.column_stack(np.where(mask == 255))\n",
    "\n",
    "    if len(white_pixels) == 0:\n",
    "        print(f\"No white pixels found in mask for index {i}, skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # Compute the mean (center location) of the white pixels\n",
    "    center_y, center_x = white_pixels.mean(axis=0)\n",
    "    return center_x, center_y\n",
    "\n",
    "for i in range(100):\n",
    "    image_path = os.path.join(images_folder, f\"{i:06d}.png\")\n",
    "    mask_path = os.path.join(masks_folder, f\"{i:06d}_predicted_mask.png\")\n",
    "\n",
    "    # Read image and predicted mask\n",
    "    image = cv2.imread(image_path)\n",
    "    predicted_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if image is None or predicted_mask is None:\n",
    "        print(f\"Missing image or mask for index {i}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    center_x, center_y = find_mask_center(predicted_mask)\n",
    "\n",
    "    # Plot image and predicted mask\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Plot the original image with the center marked\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.scatter([center_x], [center_y], color='red', s=40, label='Center')\n",
    "    plt.title(\"Original Image with Center\")\n",
    "    plt.legend()\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot the predicted mask\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_mask, cmap='gray')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define input video path and output folder\n",
    "video_path = './video.mp4'\n",
    "output_folder = './video-frames/'\n",
    "max_images = 100  # Maximum number of frames to save\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Open the video file\n",
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video was successfully opened\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Get total number of frames in the video\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Calculate the interval to capture frames\n",
    "interval = max(1, total_frames // max_images)\n",
    "\n",
    "print(f\"Total frames: {total_frames}, Saving every {interval}th frame.\")\n",
    "\n",
    "# Read and save frames\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break  # Break the loop if no more frames are available\n",
    "\n",
    "    # Save the frame only if it's at the correct interval\n",
    "    if frame_count % interval == 0:\n",
    "        output_path = os.path.join(output_folder, f\"frame_{saved_count:06d}.png\")\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        saved_count += 1\n",
    "\n",
    "        # Stop if we've saved the maximum number of frames\n",
    "        if saved_count >= max_images:\n",
    "            break\n",
    "    \n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture object\n",
    "video_capture.release()\n",
    "print(f\"Saved {saved_count} frames to {output_folder}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_circle_detection_with_mask(image, color_mask):\n",
    "    \"\"\"\n",
    "    Detect a single circular shape in the image using a color mask to focus the detection.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        color_mask (numpy.ndarray): Binary mask obtained from color thresholding.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask with the detected circle.\n",
    "    \"\"\"\n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=color_mask)\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (9, 9), 2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(blurred_image, cmap='gray')\n",
    "    plt.title(f\"Thresholding Result {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    # Detect circles using HoughCircles with adjusted parameters\n",
    "    circles = cv2.HoughCircles(\n",
    "        blurred_image,\n",
    "        cv2.HOUGH_GRADIENT,\n",
    "        dp=0.2,\n",
    "        minDist=200,\n",
    "        param1=150,\n",
    "        param2=40,\n",
    "        minRadius=30,\n",
    "        maxRadius=400,\n",
    "    )\n",
    "\n",
    "    # Create a binary mask with the same dimensions as the input image\n",
    "    mask = np.zeros_like(gray_image)\n",
    "    if circles is not None:\n",
    "        # Get the first (most prominent) circle\n",
    "        circle = np.round(circles[0, 0]).astype(\"int\")\n",
    "        x, y, r = circle\n",
    "        # Draw the circle on the mask\n",
    "        cv2.circle(mask, (x, y), r, 255, -1)\n",
    "    return mask\n",
    "\n",
    "# Load all frames and process\n",
    "frame_files = sorted([f for f in os.listdir(output_folder) if f.endswith('.png')])\n",
    "all_centers = []\n",
    "first_frame = None\n",
    "\n",
    "for idx, frame_file in enumerate(frame_files):\n",
    "    # Read the frame\n",
    "    frame_path = os.path.join(output_folder, frame_file)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    if frame is None:\n",
    "        continue\n",
    "\n",
    "    if idx == 0:\n",
    "        first_frame = frame.copy()\n",
    "\n",
    "    # Apply color thresholding\n",
    "    binary_mask = color_thresholding(frame, lower_bounds, upper_bounds)\n",
    "\n",
    "    # Apply refined circle detection (not used for plotting here, but available)\n",
    "    predicted_mask = refined_circle_detection_with_mask(frame, binary_mask)\n",
    "\n",
    "    # Find the center of the binary mask\n",
    "    center = find_mask_center(predicted_mask)\n",
    "    print(center)\n",
    "    if center:\n",
    "        all_centers.append(center)\n",
    "\n",
    "if first_frame is not None:\n",
    "    # Convert centers to integer coordinates\n",
    "    int_centers = [tuple(map(int, center)) for center in all_centers]\n",
    "\n",
    "    # Find the most left and most right centers\n",
    "    most_left = min(int_centers, key=lambda point: point[0])  # Minimum x-coordinate\n",
    "    most_right = max(int_centers, key=lambda point: point[0])  # Maximum x-coordinate\n",
    "\n",
    "    # Draw a red line between the most left and most right points\n",
    "    cv2.line(first_frame, most_left, most_right, (0, 0, 255), 2)  # Red line\n",
    "\n",
    "    # Show the result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Swing Motion Indicator (Left to Right)\")\n",
    "    plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not-real time processing\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the video\n",
    "video_path = \"video.mp4\"\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Read the first frame\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Unable to read video.\")\n",
    "\n",
    "x, y, w, h = [599, 180, 489, 449]\n",
    "if w == 0 or h == 0:\n",
    "    raise ValueError(\"No valid ROI selected.\")\n",
    "\n",
    "# Extract ROI and detect features in the first frame\n",
    "first_roi = first_frame[y:y+h, x:x+w]\n",
    "gray_first_roi = cv2.cvtColor(first_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect features in the first frame using SIFT\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(gray_first_roi, None)\n",
    "\n",
    "# Prepare for reverse frame processing\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "best_match_frame = None\n",
    "max_weighted_score = 0\n",
    "max_matches = 0\n",
    "\n",
    "# Process frames in reverse\n",
    "for i in range(frame_count - 1, -1, -10):\n",
    "    if i < 0:\n",
    "        break\n",
    "\n",
    "    print(f\"Processing frame {i}/{frame_count}\")\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    # Extract ROI from the current frame\n",
    "    roi_frame = frame[y:y+h, x:x+w]\n",
    "    gray_roi_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect features in the current frame\n",
    "    kp2, des2 = sift.detectAndCompute(gray_roi_frame, None)\n",
    "    if des2 is None:\n",
    "        continue\n",
    "\n",
    "    # Match features using FLANN\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # Apply Lowe's ratio test\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "    \n",
    "    # Compute weighted score\n",
    "    weight = i / frame_count  # Higher weight for later frames\n",
    "    weighted_score = len(good_matches) * weight\n",
    "    \n",
    "    # Track the best match\n",
    "    if weighted_score > max_weighted_score:\n",
    "        max_weighted_score = weighted_score\n",
    "        best_match_frame = i\n",
    "        print(f\"New best match at frame {best_match_frame} with {len(good_matches)} matches.\")\n",
    "\n",
    "# Calculate the rotation time\n",
    "if best_match_frame is not None:\n",
    "    rotation_time = best_match_frame / frame_rate\n",
    "    print(f\"Full rotation detected at frame {best_match_frame}. Rotation time: {rotation_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"No full rotation detected.\")\n",
    "\n",
    "# Release the video\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time video processing\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Path to the video (simulate real-time video)\n",
    "video_path = \"video.mp4\"\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_interval = 10  # Process every 10th frame\n",
    "\n",
    "# Read the first frame\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Unable to read video.\")\n",
    "\n",
    "# Define ROI (manually set or use a selection tool beforehand)\n",
    "x, y, w, h = [599, 180, 489, 449]  # Example ROI\n",
    "if w == 0 or h == 0:\n",
    "    raise ValueError(\"No valid ROI selected.\")\n",
    "\n",
    "# Extract ROI and detect features in the first frame\n",
    "first_roi = first_frame[y:y+h, x:x+w]\n",
    "gray_first_roi = cv2.cvtColor(first_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect features in the first frame using SIFT\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(gray_first_roi, None)\n",
    "\n",
    "# Initialize variables for tracking the best match\n",
    "frame_idx = 0\n",
    "best_match_frame = None\n",
    "max_weighted_score = 0\n",
    "\n",
    "# Process frames sequentially\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # End of video stream\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # Skip frames to process every 10th frame\n",
    "    if frame_idx % frame_interval != 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing frame {frame_idx}...\")\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Extract ROI from the current frame\n",
    "    roi_frame = frame[y:y+h, x:x+w]\n",
    "    gray_roi_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect features in the current frame\n",
    "    kp2, des2 = sift.detectAndCompute(gray_roi_frame, None)\n",
    "    if des2 is None:\n",
    "        continue\n",
    "\n",
    "    # Match features using FLANN\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # Apply Lowe's ratio test\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "\n",
    "    # Compute weighted score\n",
    "    weight = frame_idx / frame_rate  # Higher weight for later frames\n",
    "    weighted_score = len(good_matches) * weight\n",
    "\n",
    "    # Track the best match\n",
    "    if weighted_score > max_weighted_score:\n",
    "        max_weighted_score = weighted_score\n",
    "        best_match_frame = frame_idx\n",
    "        print(f\"New best match at frame {best_match_frame} with {len(good_matches)} matches.\")\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    elapsed = end - start\n",
    "    print(f\"Elapsed time: {elapsed} seconds\")\n",
    "\n",
    "\n",
    "# Calculate the rotation time\n",
    "if best_match_frame is not None:\n",
    "    rotation_time = best_match_frame / frame_rate\n",
    "    print(f\"Full rotation detected at frame {best_match_frame}. Rotation time: {rotation_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"No full rotation detected.\")\n",
    "\n",
    "# Release the video\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time second angle video processing\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the video (simulate real-time video)\n",
    "video_path = \"video2.mp4\"\n",
    "\n",
    "# Open the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_interval = 10  # Process every 10th frame\n",
    "\n",
    "# Read the first frame\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Unable to read video.\")\n",
    "\n",
    "# Define ROI (manually set or use a selection tool beforehand)\n",
    "x, y, w, h = [38, 112, 410, 375]  # Example ROI\n",
    "if w == 0 or h == 0:\n",
    "    raise ValueError(\"No valid ROI selected.\")\n",
    "\n",
    "# Extract ROI and detect features in the first frame\n",
    "first_roi = first_frame[y:y+h, x:x+w]\n",
    "gray_first_roi = cv2.cvtColor(first_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect features in the first frame using SIFT\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(gray_first_roi, None)\n",
    "\n",
    "# Initialize variables for tracking the best match\n",
    "frame_idx = 0\n",
    "best_match_frame = None\n",
    "max_weighted_score = 0\n",
    "\n",
    "# Process frames sequentially\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # End of video stream\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "    # Skip frames to process every 10th frame\n",
    "    if frame_idx % frame_interval != 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing frame {frame_idx}...\")\n",
    "\n",
    "    # Extract ROI from the current frame\n",
    "    roi_frame = frame[y:y+h, x:x+w]\n",
    "    gray_roi_frame = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect features in the current frame\n",
    "    kp2, des2 = sift.detectAndCompute(gray_roi_frame, None)\n",
    "    if des2 is None:\n",
    "        continue\n",
    "\n",
    "    # Match features using FLANN\n",
    "    index_params = dict(algorithm=1, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # Apply Lowe's ratio test\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "\n",
    "    # Compute weighted score\n",
    "    weight = frame_idx / frame_rate  # Higher weight for later frames\n",
    "    weighted_score = len(good_matches) * weight\n",
    "\n",
    "    # Track the best match\n",
    "    if weighted_score > max_weighted_score:\n",
    "        max_weighted_score = weighted_score\n",
    "        best_match_frame = frame_idx\n",
    "        print(f\"New best match at frame {best_match_frame} with {len(good_matches)} matches.\")\n",
    "\n",
    "# Calculate the rotation time\n",
    "if best_match_frame is not None:\n",
    "    rotation_time = best_match_frame / frame_rate\n",
    "    print(f\"Full rotation detected at frame {best_match_frame}. Rotation time: {rotation_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"No full rotation detected.\")\n",
    "\n",
    "# Release the video\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the Landing Speed in the Earth's Coordinate Frame of the Drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load video\n",
    "video_path = 'video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\n",
    "frame_time = 1 / fps  # Time per frame\n",
    "\n",
    "# Initialize variables\n",
    "ret, first_frame = cap.read()\n",
    "gray_first_frame = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect features using Shi-Tomasi or FAST\n",
    "# Shi-Tomasi Good Features to Track\n",
    "features = cv2.goodFeaturesToTrack(\n",
    "    gray_first_frame, maxCorners=100, qualityLevel=0.01, minDistance=10\n",
    ")\n",
    "features = np.int0(features)  # Convert to integer coordinates\n",
    "\n",
    "# Initialize lists for tracking\n",
    "all_positions = [features.reshape(-1, 2)]\n",
    "frame_indices = [0]\n",
    "\n",
    "# Create mask for drawing tracks\n",
    "mask = np.zeros_like(first_frame)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Optical flow calculation\n",
    "    new_features, status, _ = cv2.calcOpticalFlowPyrLK(\n",
    "        gray_first_frame, gray_frame, all_positions[-1].astype(np.float32), None\n",
    "    )\n",
    "\n",
    "    # Filter valid points\n",
    "    valid_new = new_features[status.flatten() == 1]  # Ensure status is flattened\n",
    "    valid_old = all_positions[-1][status.flatten() == 1]  # Same for the previous points\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(valid_new, valid_old)):\n",
    "        x_new, y_new = new.ravel()\n",
    "        x_old, y_old = old.ravel()\n",
    "        mask = cv2.line(mask, (int(x_new), int(y_new)), (int(x_old), int(y_old)), (0, 255, 0), 2)\n",
    "        frame = cv2.circle(frame, (int(x_new), int(y_new)), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # Add the new positions\n",
    "    all_positions.append(valid_new)\n",
    "    frame_indices.append(frame_indices[-1] + 1)\n",
    "\n",
    "    # Update for next iteration\n",
    "    gray_first_frame = gray_frame\n",
    "\n",
    "    # Show the frame with tracks\n",
    "    cv2.imshow('Frame', cv2.add(frame, mask))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Analyze motion for full rotation\n",
    "first_positions = all_positions[0]\n",
    "rotation_times = []\n",
    "for i, positions in enumerate(all_positions):\n",
    "    displacement = np.linalg.norm(positions - first_positions, axis=1)\n",
    "    if np.mean(displacement) < 5:  # Threshold for detecting a return to initial positions\n",
    "        rotation_times.append(i * frame_time)\n",
    "\n",
    "# Calculate rotation times\n",
    "rotation_durations = np.diff(rotation_times)\n",
    "print(f\"Average Rotation Time: {np.mean(rotation_durations):.2f} seconds\")\n",
    "\n",
    "# Plot feature tracks\n",
    "plt.figure(figsize=(8, 8))\n",
    "for positions in all_positions:\n",
    "    plt.scatter(positions[:, 0], positions[:, 1], s=1)\n",
    "plt.title('Feature Tracks')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
