{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct the distorted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def align_images_multiple_references(reference_image_paths, distorted_image_path):\n",
    "    # Load the distorted image\n",
    "    dist_img = cv2.imread(distorted_image_path)\n",
    "    dist_gray = cv2.cvtColor(dist_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Aggregate keypoints and descriptors from all reference images\n",
    "    ref_keypoints = []\n",
    "    ref_descriptors = []\n",
    "    for ref_path in reference_image_paths:\n",
    "        ref_img = cv2.imread(ref_path)\n",
    "        ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)\n",
    "        keypoints, descriptors = sift.detectAndCompute(ref_gray, None)\n",
    "        ref_keypoints.extend(keypoints)\n",
    "        if descriptors is not None:\n",
    "            if len(ref_descriptors) == 0:\n",
    "                ref_descriptors = descriptors\n",
    "            else:\n",
    "                ref_descriptors = np.vstack((ref_descriptors, descriptors))\n",
    "\n",
    "    # Detect keypoints and descriptors in the distorted image\n",
    "    keypoints_dist, descriptors_dist = sift.detectAndCompute(dist_gray, None)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(ref_descriptors, descriptors_dist)\n",
    "\n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    ref_pts = np.float32([ref_keypoints[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dist_pts = np.float32([keypoints_dist[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate homography\n",
    "    matrix, mask = cv2.findHomography(dist_pts, ref_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp the distorted image\n",
    "    h, w = dist_img.shape[:2]\n",
    "    aligned_image = cv2.warpPerspective(dist_img, matrix, (w, h))\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Distorted Image\")\n",
    "    plt.imshow(cv2.cvtColor(dist_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Aligned Image\")\n",
    "    plt.imshow(cv2.cvtColor(aligned_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def transform_image_with_homography(image_path, homography_matrix):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w = image.shape[:2]  # Get the size from the image itself\n",
    "\n",
    "    # Apply the homography transformation\n",
    "    transformed_image = cv2.warpPerspective(image, homography_matrix, (w, h))\n",
    "\n",
    "    return transformed_image\n",
    "\n",
    "def reverse_transform_image_with_homography(transformed_image, homography_matrix):\n",
    "    \"\"\"\n",
    "    Reverse the homography transformation on an image.\n",
    "\n",
    "    Parameters:\n",
    "        transformed_image: The image that was transformed.\n",
    "        homography_matrix: The homography matrix used for the forward transformation.\n",
    "\n",
    "    Returns:\n",
    "        The reversed (original) image.\n",
    "    \"\"\"\n",
    "    # Calculate the inverse of the homography matrix\n",
    "    inverse_homography_matrix = np.linalg.inv(homography_matrix)\n",
    "    \n",
    "    # Get the size of the transformed image\n",
    "    h, w = transformed_image.shape[:2]\n",
    "    \n",
    "    # Apply the inverse homography transformation\n",
    "    original_image = cv2.warpPerspective(transformed_image, inverse_homography_matrix, (w, h))\n",
    "    \n",
    "    return original_image\n",
    "\n",
    "def save_homography_matrix(matrix, file_path):\n",
    "    # Convert the NumPy array to a list for JSON serialization\n",
    "    matrix_list = matrix.tolist()\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(matrix_list, f)\n",
    "\n",
    "def process_dataset_images(dataset_folder):\n",
    "    images_folder = os.path.join(dataset_folder, \"images\")\n",
    "    \n",
    "    for image_name in os.listdir(images_folder):\n",
    "        print(image_name)\n",
    "        image_path = os.path.join(images_folder, image_name)\n",
    "\n",
    "        reference_image_paths = [\n",
    "            './Dataset 2/images/000003.png', './Dataset 2/images/000010.png', './Dataset 2/images/000016.png', './Dataset 2/images/000021.png', './Dataset 2/images/000022.png', './Dataset 2/images/000028.png', './Dataset 2/images/000030.png', './Dataset 2/images/000033.png', './Dataset 2/images/000046.png', './Dataset 2/images/000051.png', './Dataset 2/images/000053.png', './Dataset 2/images/000060.png', './Dataset 2/images/000061.png', './Dataset 2/images/000064.png',\n",
    "            './Dataset 2/images/000072.png', './Dataset 2/images/000081.png', './Dataset 2/images/000096.png', \n",
    "        ]  # Add more references\n",
    "        #distorted_image_path = './Dataset 2/images/000083.png'\n",
    "        homography_matrix = align_images_multiple_references(reference_image_paths, image_path)\n",
    "\n",
    "        # Transform the image\n",
    "        transformed_image = transform_image_with_homography(image_path, homography_matrix)\n",
    "\n",
    "        if not cv2.imwrite(f\"./calibrated-images/{image_name}\", transformed_image):\n",
    "            print(f\"Failed to save image: {image_name}\")\n",
    "\n",
    "        save_homography_matrix(homography_matrix, f\"./calibrated-images/{image_name.replace('png', 'json')}\")\n",
    "\n",
    "        # Plot the original and transformed images\n",
    "        #original_image = cv2.imread(image_path)\n",
    "        #plt.figure(figsize=(12, 6))\n",
    "        #plt.subplot(1, 2, 1)\n",
    "        #plt.title(\"Original Image\")\n",
    "        #plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
    "        #plt.axis(\"off\")\n",
    "\n",
    "        #plt.subplot(1, 2, 2)\n",
    "        #plt.title(\"Transformed Image\")\n",
    "        #plt.imshow(cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "        #plt.axis(\"off\")\n",
    "\n",
    "        #plt.tight_layout()\n",
    "        #plt.show()\n",
    "\n",
    "# Example usage\n",
    "dataset_folder = './Dataset 2'  # Replace with the actual dataset folder\n",
    "process_dataset_images(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "import json\n",
    "\n",
    "def load_homography_matrix(file_path):\n",
    "    # Load the JSON file and convert the list back to a NumPy array\n",
    "    with open(file_path, 'r') as f:\n",
    "        matrix_list = json.load(f)\n",
    "    return np.array(matrix_list)\n",
    "\n",
    "def reverse_transform_image_with_homography(transformed_image, homography_matrix):\n",
    "    \"\"\"\n",
    "    Reverse the homography transformation on an image.\n",
    "\n",
    "    Parameters:\n",
    "        transformed_image: The image that was transformed.\n",
    "        homography_matrix: The homography matrix used for the forward transformation.\n",
    "\n",
    "    Returns:\n",
    "        The reversed (original) image.\n",
    "    \"\"\"\n",
    "    # Calculate the inverse of the homography matrix\n",
    "    inverse_homography_matrix = np.linalg.inv(homography_matrix)\n",
    "    \n",
    "    # Get the size of the transformed image\n",
    "    h, w = transformed_image.shape[:2]\n",
    "    \n",
    "    # Apply the inverse homography transformation\n",
    "    original_image = cv2.warpPerspective(transformed_image, inverse_homography_matrix, (w, h))\n",
    "    \n",
    "    return original_image\n",
    "\n",
    "def color_thresholding(image, lower_bounds, upper_bounds):\n",
    "    \"\"\"\n",
    "    Perform color thresholding to segment the globe.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        lower_bounds (list of numpy.ndarray): List of lower HSV color bounds.\n",
    "        upper_bounds (list of numpy.ndarray): List of upper HSV color bounds.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask highlighting the selected colors.\n",
    "    \"\"\"\n",
    "    # Convert image to HSV color space\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = np.zeros(hsv_image.shape[:2], dtype=np.uint8)\n",
    "    # Apply each color threshold and combine the masks\n",
    "    for lower_bound, upper_bound in zip(lower_bounds, upper_bounds):\n",
    "        mask |= cv2.inRange(hsv_image, lower_bound, upper_bound)\n",
    "    return mask\n",
    "\n",
    "def refined_circle_detection_with_mask(image, color_mask):\n",
    "    \"\"\"\n",
    "    Detect a single circular shape in the image using a color mask to focus the detection.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image in BGR format.\n",
    "        color_mask (numpy.ndarray): Binary mask obtained from color thresholding.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask with the detected circle.\n",
    "    \"\"\"\n",
    "    # Apply the mask to the image\n",
    "    masked_image = cv2.bitwise_and(image, image, mask=color_mask)\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (9, 9), 2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(blurred_image, cmap='gray')\n",
    "    plt.title(f\"Thresholding Result {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    # Detect circles using HoughCircles with adjusted parameters\n",
    "    #circles = cv2.HoughCircles(\n",
    "    #    blurred_image,\n",
    "    #    cv2.HOUGH_GRADIENT_ALT,\n",
    "    #    dp=1.2,\n",
    "    #    minDist=200,\n",
    "    #    param1=100,\n",
    "    #    param2=40,\n",
    "    #    minRadius=200,\n",
    "    #    maxRadius=500,\n",
    "    #)\n",
    "    \n",
    "    circles = cv2.HoughCircles(\n",
    "        blurred_image,\n",
    "        cv2.HOUGH_GRADIENT_ALT,\n",
    "        dp=2.3,          # Resolution of the accumulator (adjust if necessary)\n",
    "        minDist=200,     # Minimum distance between detected circle centers\n",
    "        param1=200,      # Higher threshold for Canny edge detection\n",
    "        param2=0.30,      # Threshold for circle detection in accumulator\n",
    "        minRadius=55,   # Minimum circle radius (adapt based on the image)\n",
    "        maxRadius=444    # Maximum circle radius\n",
    "    )\n",
    "    # Create a binary mask with the same dimensions as the input image\n",
    "    mask = np.zeros_like(gray_image)\n",
    "    if circles is not None:\n",
    "        # Get the first (most prominent) circle\n",
    "        circle = np.round(circles[0, 0]).astype(\"int\")\n",
    "        x, y, r = circle\n",
    "        # Draw the circle on the mask\n",
    "        cv2.circle(mask, (x, y), r, 255, -1)\n",
    "    return mask\n",
    "\n",
    "def compute_roc_curve(predicted_mask, ground_truth_mask):\n",
    "    \"\"\"\n",
    "    Compute ROC curve and AUC for a predicted mask and ground truth mask.\n",
    "\n",
    "    Parameters:\n",
    "        predicted_mask (numpy.ndarray): Predicted binary mask.\n",
    "        ground_truth_mask (numpy.ndarray): Ground truth binary mask.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (fpr, tpr, roc_auc) False Positive Rate, True Positive Rate, and Area Under Curve.\n",
    "    \"\"\"\n",
    "    # Binarize the masks\n",
    "    predicted_binary = (predicted_mask > 0).astype(np.uint8).flatten()\n",
    "    ground_truth_binary = (ground_truth_mask > 0).astype(np.uint8).flatten()\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, _ = roc_curve(ground_truth_binary, predicted_binary)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "# Dataset paths\n",
    "images_folder = './Dataset 2/images/'\n",
    "corrected_images_folder = './calibrated-images/'\n",
    "masks_folder = './Dataset 2/masks/'\n",
    "\n",
    "# Bounds for color thresholding\n",
    "lower_bounds = [\n",
    "    np.array([100, 50, 50]),  # Lower bound of blue in HSV\n",
    "    #np.array([0, 0, 200]),    # Lower bound of white in HSV\n",
    "]\n",
    "upper_bounds = [\n",
    "    np.array([140, 255, 255]),  # Upper bound of blue in HSV\n",
    "    #np.array([180, 50, 255]),   # Upper bound of white in HSV\n",
    "]\n",
    "\n",
    "# Lists to store ROC values\n",
    "all_fpr = []\n",
    "all_tpr = []\n",
    "all_auc = []\n",
    "\n",
    "# Loop through all images and masks\n",
    "for i in range(100):\n",
    "    image_path = os.path.join(images_folder, f\"{i:06d}.png\")\n",
    "    corrected_image_path = os.path.join(corrected_images_folder, f\"{i:06d}.png\")\n",
    "    mask_path = os.path.join(masks_folder, f\"{i:06d}.png\")\n",
    "\n",
    "    # Read image and ground truth mask\n",
    "    image = cv2.imread(image_path)\n",
    "    corrected_image = cv2.imread(corrected_image_path)\n",
    "    ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    loaded_matrix = load_homography_matrix(corrected_image_path.replace(\"png\", \"json\"))\n",
    "\n",
    "    # Generate color mask\n",
    "    color_mask = color_thresholding(corrected_image, lower_bounds, upper_bounds)\n",
    "\n",
    "    # Plot the thresholding result\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(color_mask, cmap='gray')\n",
    "    plt.title(f\"Thresholding Result {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Refine circle detection with color mask\n",
    "    predicted_mask = reverse_transform_image_with_homography(refined_circle_detection_with_mask(image, color_mask), loaded_matrix)\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, roc_auc = compute_roc_curve(predicted_mask, ground_truth_mask)\n",
    "\n",
    "    # Store the results\n",
    "    all_fpr.append(fpr)\n",
    "    all_tpr.append(tpr)\n",
    "    all_auc.append(roc_auc)\n",
    "\n",
    "    # Plot image, ground truth mask, and predicted mask\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Image {i:06d}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(ground_truth_mask, cmap='gray')\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(predicted_mask, cmap='gray')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute average ROC curve (mean TPR for each FPR)\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for fpr, tpr in zip(all_fpr, all_tpr):\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "mean_tpr /= len(all_tpr)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "\n",
    "# Plot average ROC curve\n",
    "plt.figure()\n",
    "plt.plot(mean_fpr, mean_tpr, label=f\"Mean ROC (AUC = {mean_auc:.2f})\")\n",
    "plt.title(\"Average ROC Curve for Test Set\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Print mean AUC\n",
    "print(f\"Mean AUC: {mean_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code to detect an arrow (seven-sided shape) from an image. \n",
    "import numpy as np \n",
    "import cv2 \n",
    "\n",
    "# Reading image \n",
    "img2 = cv2.imread('./calibrated-images/000000.png', cv2.IMREAD_COLOR) \n",
    "\n",
    "# Reading same image in another variable and \n",
    "# converting to gray scale. \n",
    "img = cv2.imread('./calibrated-images/000000.png', cv2.IMREAD_GRAYSCALE) \n",
    "\n",
    "# Converting image to a binary image \n",
    "# (black and white only image). \n",
    "_,threshold = cv2.threshold(img, 110, 255, \n",
    "\t\t\t\t\t\t\tcv2.THRESH_BINARY) \n",
    "\n",
    "# Detecting shapes in image by selecting region \n",
    "# with same colors or intensity. \n",
    "contours,_=cv2.findContours(threshold, cv2.RETR_TREE, \n",
    "\t\t\t\t\t\t\tcv2.CHAIN_APPROX_SIMPLE) \n",
    "\n",
    "# Searching through every region selected to \n",
    "# find the required polygon. \n",
    "for cnt in contours : \n",
    "\tarea = cv2.contourArea(cnt) \n",
    "\n",
    "\t# Shortlisting the regions based on there area. \n",
    "\tif area > 400: \n",
    "\t\tapprox = cv2.approxPolyDP(cnt, \n",
    "\t\t\t\t\t\t\t\t0.009 * cv2.arcLength(cnt, True), True) \n",
    "\n",
    "\t\t# Checking if the no. of sides of the selected region is 7. \n",
    "\t\tif(len(approx) == 7): \n",
    "\t\t\tcv2.drawContours(img2, [approx], 0, (0, 0, 255), 5) \n",
    "\n",
    "# Showing the image along with outlined arrow. \n",
    "cv2.imshow('image2', img2) \n",
    "\n",
    "# Exiting the window if 'q' is pressed on the keyboard. \n",
    "if cv2.waitKey(0) & 0xFF == ord('q'): \n",
    "\tcv2.destroyAllWindows() \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0241",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
